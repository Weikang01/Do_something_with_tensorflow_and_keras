{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parallel_computing_with_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkBw1KgtVD+sr85pTSG7Z9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weikang01/Do_something_with_tensorflow_and_keras/blob/master/parallel_computing_with_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzMbScfb1lBk",
        "colab_type": "text"
      },
      "source": [
        "本文是对https://www.youtube.com/watch?v=rj-hjS5L8Bw 所做的笔记\n",
        "#**Tensorflow Strategies on parallel computing**#\n",
        "TensorFlow上的并行计算框架都在tensorflow.distribute库中\n",
        "```python\n",
        "import tensorflow.distribute\n",
        "```\n",
        "<b>Tensorflow</b>一共有六种并行计算框架，用户需要根据设备\n",
        "* **Mirror Strategy**\n",
        "<p>适合一台服务器拥有多块GPU</p>\n",
        "<p>Mirror Strategy类似于MapReduce编程模型，每块GPU都拥有完整的模型参数，每块GPU用1块batch的数据计算随机梯度，然后把所有GPU算出来的随机梯度求和，用随机梯度加和来更新模型参数</p>\n",
        "<p>Mirror Strategy做的是同步随机梯度下降，需要等到所有GPU全部完成计算</p>\n",
        "* **TPU Strategy**\n",
        "* **Multi-Worker Mirrored Strategy**\n",
        "* **Central Storage Strategy**\n",
        "* **Parameter Server Strategy**\n",
        "* **One Device Strategy**\n",
        "#**Parallel Training CNN on MNIST**#\n",
        "用并行网络来训练一个CNN来预测手写数字数据集mnist\n",
        "* 需要安装TensorFlow的GPU版本\n",
        "```python\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "```\n",
        "<p>/device:CPU:0</p>\n",
        "<p>/device:GPU:0</p>\n",
        "<p>/device:GPU:1</p>\n",
        "<p>/device:GPU:2</p>\n",
        "<p>/device:GPU:3</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJK9Kkgp2um_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVK32JZz6YZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.distribute as distribute\n",
        "strategy = distribute.MirroredStrategy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8UPl3Yt7Jbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BJxcTIf7rUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f5a4e97-c7af-4a81-e9cf-14320109dcc0"
      },
      "source": [
        "import tensorflow_datasets as data\n",
        "datasets, info = data.load(name='mnist', with_info=True, as_supervised=True)\n",
        "mnist_train = datasets['train'].map(scale).cache()\n",
        "mnist_test = datasets['test'].map(scale)\n",
        "m = strategy.num_replicas_in_sync  # 得到当前Processor的数量\n",
        "print(m)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2O0izw88UKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE_PER_REPLICA = 128\n",
        "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * m\n",
        "data_train = mnist_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "data_test = mnist_test.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN-0GIFUH6Lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "with strategy.scope():  # That's what you need to do to run in parallale computing\n",
        "  model = Sequential(name='hello')\n",
        "  model.add(Conv2D(32, 3, activation='relu', input_shape=(28,28,1)))\n",
        "  model.add(MaxPooling2D())\n",
        "  model.add(Conv2D(64, 3, activation='relu'))\n",
        "  model.add(MaxPooling2D())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48eVx52wKyoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "         optimizer=keras.optimizers.RMSprop(learning_rate=1E-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYJPoIvMwCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "00e794ad-a4ef-4477-e515-0b0ac9d20fd8"
      },
      "source": [
        "model.fit(data_train, epochs=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 47s 101ms/step - loss: 0.2159\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 41s 88ms/step - loss: 0.0587\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0406\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0314\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0238\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0190\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 42s 89ms/step - loss: 0.0147\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 41s 88ms/step - loss: 0.0123\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 41s 88ms/step - loss: 0.0102\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0ff9c1cfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwnEGPFAO4QC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_loss, eval_acc = model.evaluate(data_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUi2j-UNPOi2",
        "colab_type": "text"
      },
      "source": [
        "##**Mirror Strategy的运行原理**##\n",
        "* Ring All-Reduce<br>\n",
        "<b>Reduce和All-Reduce的区别</b><br>\n",
        "Reduce: Server对从worker得到的计算结果进行计算（如：reduce_sum，reduce_mean...）<br>\n",
        "如：<br>\n",
        "$$output_{worker1}=7$$\n",
        "$$output_{worker2}=3$$\n",
        "$$output_{worker3}=4$$\n",
        "$$output_{worker4}=1$$\n",
        "$$Reduce(sum) = \\sum^n_{i=1}output_{worker_i}=7+3+4+1=15$$<br>\n",
        "* 在这种情况下，Server得到reduce的结果，但各个worker并不知道\n",
        "* All-Reduce：所有节点都会获得reduce结果的一个副本\n",
        "> E.g.,通过reduce+broadcast实现all-reduce<br>\n",
        "> E.g.,通过all-to-all communication实现all-reduce（worker之间互相向其余所有节点传递自身信息，没有了server的参与）<br>\n",
        "> E.g.,通过ring all-reduce\n",
        "* **Naïve Ring All-Reduce Algorithm朴素环全约简算法**<br>\n",
        "逻辑：<br>\n",
        "$$GPU_1:g_1\\longrightarrow GPU_2$$\n",
        "$$GPU_2:(g_1+g_2)\\longrightarrow GPU_3$$\n",
        "$$...$$\n",
        "$$GPU_{m-1}:(\\sum^{m-1}_{i=1}g_i)\\longrightarrow GPU_m$$\n",
        "通过传递的方法GPU(m)获得了加和的数据，之后从GPU(m)开始再进行循环，使所有GPU均获得完整的梯度g。\n",
        "$$GPU_m:g\\Longrightarrow GPU_1$$\n",
        "$$GPU_1:g\\Longrightarrow GPU_2$$\n",
        "$$...$$\n",
        "$$GPU_{m-2}:g\\Longrightarrow GPU_{m-1}$$\n",
        "这种算法的问题是：\n",
        "* 同一时间大多数的GPU都是闲置的\n",
        "* 通信时间：md/b\n",
        " * m：GPU数量\n",
        " * d:参数数量\n",
        " * b:网络带宽\n",
        "* **Ring All-Reduce Algorithm优化的环全约简算法**<br>\n",
        "\n",
        "\n",
        "1. 将参数切分为m份，m为GPU数量:\n",
        "$$GPU_0: g_0=[a_0;b_0;c_0;...]$$\n",
        "$$GPU_1: g_1=[a_1;b_1;c_1;...]$$\n",
        "$$...$$\n",
        "$$GPU_m: g_m=[a_m;b_m;c_m;...]$$\n",
        "2. GPU之间同时向同一方向通信，同时发送自身参数的第i个参数部分发向i+1号GPU（m号发送给1号GPU）\n",
        "$$GPU_1: a_0\\longrightarrow GPU_2$$\n",
        "$$GPU_2: b_1\\longrightarrow GPU_3$$\n",
        "$$...$$\n",
        "$$GPU_m: x_m\\longrightarrow GPU_1$$\n",
        "3. 此时，i+1号GPU可以计算出i参数和i+1的加和。\n",
        "4. 以此类推，可以以朴素方法m倍的效率完成计算。\n",
        "##<b>算法特点总结</b>\n",
        "* 没有闲置的计算机网络\n",
        "* 通信时间：d/b\n",
        " * d:参数数量\n",
        " * b:网络带宽\n",
        "\n",
        "\n"
      ]
    }
  ]
}